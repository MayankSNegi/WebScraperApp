# ğŸ“š WebScraperApp

A **full-stack web scraping application** built using **Python (Flask)** and **BeautifulSoup**, designed to extract, visualize, and manage structured data from [Books to Scrape](https://books.toscrape.com).

---

## ğŸŒ What is a Web Scraper?

A **web scraper** is an automated system that extracts useful information from websites.  
It works by requesting a web page, reading its HTML content, and filtering out relevant data fields.

In essence:
> **Web Scraping = Data Extraction Automation from the Web**

Organizations, researchers, and analysts use scrapers to collect **large-scale public data** for insights, product tracking, and decision-making â€” replacing manual data collection with fast, repeatable automation.

---

## ğŸ¤– What This Scraper Does

**WebScraperApp** targets the open dataset site **Books to Scrape**, a sandbox for testing web data extraction.  
It automatically collects and organizes structured book details such as:

- ğŸ“˜ **Title**  
- ğŸ’° **Price**  
- ğŸ·ï¸ **Availability**  
- â­ **Rating**

After extraction, the data is:
- Saved to a **CSV file (`data/scraped_data.csv`)**
- Displayed dynamically in an **interactive web interface**
- Made available for **download and further analytics**

---

## ğŸš€ Key Features (Action + What + How + Impact)

| # | Action | What | How | Impact / Application |
|---|---------|------|------|----------------------|
| 1ï¸âƒ£ | **Developed** | a full-stack web scraping application | using Flask backend, BeautifulSoup parser, and JS frontend | Enables automated data collection and integration into analytics systems |
| 2ï¸âƒ£ | **Extracted** | structured data fields from HTML content | by parsing the DOM structure with BeautifulSoup | Transforms unstructured web content into machine-readable formats |
| 3ï¸âƒ£ | **Implemented** | RESTful APIs for scraping, data access, and download | using Flask and Flask-CORS | Facilitates interoperability between frontend systems and backend services |
| 4ï¸âƒ£ | **Automated** | data storage workflow | with CSV serialization and read/write operations | Provides persistent, portable datasets for offline analysis |
| 5ï¸âƒ£ | **Designed** | a responsive web interface | with HTML5, CSS3, and vanilla JS | Improves data accessibility and visualization across devices |
| 6ï¸âƒ£ | **Ensured** | robust error handling | via exception blocks in scraper logic | Increases system reliability and fault tolerance during runtime |
| 7ï¸âƒ£ | **Structured** | modular project architecture | separating backend, frontend, and data layers | Supports maintainability and scalable codebase expansion |
| 8ï¸âƒ£ | **Enabled** | automatic browser launching | via Pythonâ€™s `webbrowser` and threading | Enhances usability and testing convenience |

---

## ğŸ§  Technologies Used

| Layer | Technology | Purpose |
|-------|-------------|----------|
| **Backend** | Flask (Python), Flask-CORS | REST API, data routing, static serving |
| **Scraping Engine** | BeautifulSoup4, requests | HTML parsing & extraction |
| **Frontend** | HTML5, CSS3, JavaScript | Visualization and interactivity |
| **Data Layer** | CSV (Python `csv` module) | Persistent storage of structured data |
| **Environment** | venv (Virtual Environment) | Dependency isolation |

---

## ğŸ“ Project Structure
WebScraperApp/<br>
â”œâ”€â”€ backend/<br>
â”‚ â”œâ”€â”€ venv/ â† Python virtual environment<br>
â”‚ â”œâ”€â”€ init.py<br>
â”‚ â”œâ”€â”€ app.py â† Flask app (main entry)<br>
â”‚ â”œâ”€â”€ scraper.py â† Scraper logic (BeautifulSoup)<br>
â”‚ â”œâ”€â”€ data_handler.py â† CSV read/write handler<br>
â”‚ â””â”€â”€ requirements.txt â† Dependencies<br>
â”‚<br>
â”œâ”€â”€ frontend/<br>
â”‚ â”œâ”€â”€ index.html â† Dashboard UI<br>
â”‚ â”œâ”€â”€ style.css â† Styling <br>
â”‚ â””â”€â”€ script.js â† API & interactivity logic<br>
â”‚<br>
â”œâ”€â”€ data/<br>
â”‚ â””â”€â”€ scraped_data.csv â† Generated output file<br>
â””â”€â”€ README.md
---

## âš™ï¸ Setup & Installation

### **1ï¸âƒ£ Clone the Repository**
- git clone https://github.com/your-username/WebScraperApp.git
- cd WebScraperApp
### **2ï¸âƒ£ Create Virtual Environment (inside backend/)**
- cd backend
- python -m venv venv
- Activate it:
- - Windows: venv\Scripts\activate
- - macOS/Linux: source venv/bin/activate
### **3ï¸âƒ£ Install Dependencies**
- pip install -r requirements.txt
### **4ï¸âƒ£ Run the App**
- python app.py
âœ… Browser opens automatically at
ğŸ‘‰ http://127.0.0.1:5000
 
---

## ğŸ’» Usage
- Click ğŸ”„ Start Scraping to begin extraction
- Wait until scraping finishes (logs appear in terminal)
- View all books in the table below
- Click ğŸ“¥ Download CSV to save the dataset locally

---

## ğŸ” Understanding How It Works
- 1ï¸âƒ£ Frontend UI
- â†’ Displays data and user controls using HTML/CSS/JS
- 2ï¸âƒ£ Flask Backend
- â†’ Handles /api/scrape, /api/data, /api/download routes
- 3ï¸âƒ£ Scraper Module (scraper.py)
- â†’ Sends HTTP requests and parses the HTML
- â†’ Extracts required fields
- 4ï¸âƒ£ Data Handler (data_handler.py)
- â†’ Saves extracted results into scraped_data.csv
- â†’ Reads existing CSV data when reloading page

---

## ğŸ§± Future Enhancements
- Integrate with a database (SQLite / MongoDB)
- Add data visualization (Chart.js or Plotly)
- Enable scheduled scraping (cron jobs)
- Add multiple target sites
- Deploy to cloud (Render / Railway / Vercel)

---

## ğŸ Conclusion

- WebScraperApp encapsulates the essence of data automation, backend engineering, and frontend integration.<br>
- It bridges the gap between raw web content and structured, analyzable data â€” a core component of modern data intelligence systems.<br>
- This project exemplifies how small-scale automation can have large-scale impact in research, analytics, and digital operations.<br>

---

## ğŸ‘¨â€ğŸ’» Author
### [Mayank Singh Negi](https://github.com/MayankSNegi)